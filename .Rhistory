mutate(corpus = removeWords(corpus, stopwords("es"))) %>%
mutate(corpus = gsub("\\d+", " ", corpus)) %>%
mutate(corpus = gsub("http\\S+", " ", corpus)) %>%
mutate(corpus = gsub("www\\S+", " ", corpus)) %>%
mutate(corpus = gsub("\\b\\w{1,2}\\b", " ", corpus)) %>%
mutate(corpus = gsub("\\s+", " ", corpus))
View(train_clean)
train_clean <- train_clean %>% mutate(id = row_number())
train_clean <- train %>%
mutate(corpus = stri_trans_general(text, id = "Latin-ASCII")) %>%
mutate(corpus = removeNumbers(text)) %>%
mutate(corpus = removePunctuation(corpus)) %>%
mutate(corpus = tolower(corpus)) %>%
mutate(corpus = stripWhitespace(corpus)) %>%
mutate(corpus = str_replace_all(corpus, "[^[:alnum:]\\s]", ""))  %>%
mutate(corpus = gsub("[^[:alpha:]#\\s]", " ", corpus)) %>%
mutate(corpus = removeWords(corpus, stopwords("es"))) %>%
mutate(corpus = gsub("\\d+", " ", corpus)) %>%
mutate(corpus = gsub("http\\S+", " ", corpus)) %>%
mutate(corpus = gsub("www\\S+", " ", corpus)) %>%
mutate(corpus = gsub("\\b\\w{1,2}\\b", " ", corpus)) %>%
mutate(corpus = gsub("\\s+", " ", corpus))
?unnest_tokens
train_token <- train_clean %>% unnest_tokens("word", corpus)
View(train_token)
train_token %>% count(word, sort = TRUE) %>% head()
sw <- c()
for (s in c("snowball", "stopwords-iso", "nltk")) {
temp <- get_stopwords("spanish", source = s)$word
sw <- c(sw, temp)
}
sw <- unique(sw)
sw <- unique(stri_trans_general(str = sw, id = "Latin-ASCII"))
sw <- data.frame(word = sw)
sw <- c()
for (s in c("snowball", "stopwords-iso", "nltk")) {
temp <- get_stopwords("spanish", source = s)$word
sw <- c(sw, temp)
}
sw <- unique(sw)
sw <- unique(stri_trans_general(str = sw, id = "Latin-ASCII"))
sw <- data.frame(word = sw)
sw
nrow(train_token)
train_token <- train_token %>% anti_join(sw, by = "word")
nrow(train_token)
autor_top_word <- train_cloud %>%
group_by(name) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
arrange(name, -n)
train_cloud <- train_token %>% count(name, word) %>% group_by(name) %>% ungroup()
train_cloud <- train_token %>% count(name, word) %>% group_by(name) %>% ungroup()
autor_top_word <- train_cloud %>%
group_by(name) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
arrange(name, -n)
imagen_top_autor <- ggplot(autor_top_word, aes(y = reorder(word, n), x = n, fill = factor(name))) +
geom_bar(stat = "identity") +
ggtitle("Términos más frecuentes") +
ylab("Términos") +
facet_wrap(~ name, scales = "free") +
xlab("Frecuencia")
imagen_top_autor
imagen_top_autor <- ggplot(autor_top_word, aes(y = reorder(word, n), x = n, fill = factor(name))) +
geom_bar(stat = "identity") +
ggtitle("Términos más frecuentes") +
ylab("Términos") +
facet_wrap(~ name, scales = "free") +
xlab("Frecuencia") +
theme(legend.position = "none")
imagen_top_autor
train_cloud <- train_token %>% count(name, word) %>% group_by(name) %>% ungroup()
table(train_cloud$name) %>% as.data.frame()
cloud_lopez <- train_cloud %>% filter(name == "Lopez") %>% head(50)
table(train_cloud$name) %>% as.data.frame()
cloud_lopez <- train_cloud %>% filter(name == "Lopez") %>% head(50)
cloud_uribe <- train_cloud %>% filter(name == "Uribe") %>% head(50)
cloud_petro <- train_cloud %>% filter(name == "Petro") %>% head(50)
wordcloud(train_cloud$word, freq = train_cloud$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(train_cloud$word, freq = train_cloud$n, min.freq = 80, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(cloud_lopez$word, freq = cloud_lopez$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
View(cloud_petro)
View(train_cloud)
cloud_lopez <- train_cloud %>% filter(name == "Lopez") %>% arrange(desc(n)) %>% head(50)
cloud_uribe <- train_cloud %>% filter(name == "Uribe") %>% arrange(desc(n)) %>% head(50)
cloud_petro <- train_cloud %>% filter(name == "Petro") %>% arrange(desc(n)) %>% head(50)
wordcloud(train_cloud$word, freq = train_cloud$n, min.freq = 80, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(cloud_lopez$word, freq = cloud_lopez$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(cloud_uribe$word, freq = cloud_uribe$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(cloud_petro$word, freq = cloud_petro$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
View(cloud_lopez)
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
udpipe::udpipe_download_model('spanish')
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- train_token %>% distinct(train_token$word)
View(palabras_unicas)
palabras_unicas <- train_token %>% distinct(train_token)
palabras_unicas <- train_token %>% distinct(train_token$word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
View(palabras_unicas)
palabras_unicas <- train_token %>% distinct(palabras_unicas = train_token$word)
palabras_unicas <- train_token %>% distinct(word = train_token$word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- train_token %>% distinct(word = train_token$word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
spanish-gsd-ud-2.5-191206.udpipe
udpipe::udpipe_download_model('spanish')
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- train_token %>% distinct(word = train_token$word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
udpipe_results <- as_tibble(udpipe_results)
udpipe_results <- udpipe_results %>% select(token, lemma) %>% rename("word" = "token")
train_token <- train_token %>% left_join(udpipe_results, by = "word", multiple = "all")
train_token[is.na(train_token$lemma), "lemma"] <- train_token[is.na(train_token$lemma), "word"]
train_token %>% count(lemma) %>% arrange(desc(n)) %>% tail(100)
palabras_eliminar <- train_token %>% count(lemma) %>% filter(n < 10)
View(palabras_eliminar)
train_token <- train_token %>% anti_join(palabras_eliminar, by = "lemma")
View(train_token)
train_clean_2 <- train_token %>%
group_by(name, id) %>%
summarise(text = str_c(lemma, collapse = " ")) %>%
ungroup()
setdiff(train_clean$id, train_clean_2$id)
setdiff(train_clean$id, train_clean_2$id) %>%as.data.frame()
data[c(f6b905f08777336f17429244, 26f1d4be2b396e2e77fbe450),]
data[c("f6b905f08777336f17429244", "26f1d4be2b396e2e77fbe450"),]
train_clean[c("f6b905f08777336f17429244", "26f1d4be2b396e2e77fbe450"),]
train_clean_2[c("f6b905f08777336f17429244", "26f1d4be2b396e2e77fbe450"),]
train_clean %>% filter (id == "f6b905f08777336f17429244")
diferencia <- setdiff(train_clean$id, train_clean_2$id) %>% as.data.frame()
dif <- dim(diferencia)
diferencia
dif
dif <- nrow(diferencia)
dif
inicial <- nrow(train_clean)
final <- nrow(train_clean_2)
inicial - final
train_clean %>% select(name, corpus) %>% filter (id == "339f76c5c22f1cc9faaa9075")
train_clean %>% filter (id == "339f76c5c22f1cc9faaa9075")
train_clean %>% filter (id == "ce504d0083b733f0e9be4b00")
train_clean %>% filter (id == "ae0cfc8f52d6d7e603a67e88")
train_clean %>% filter (id == "ce1464da0f03a61f2659947b") #Ejemplo para validar
tm_corpus <- Corpus(VectorSource(x = train_clean_2$text))
str(tm_corpus)
tf_idf <- TermDocumentMatrix(tm_corpus, control = list(weighting = weightTfIdf))
tf_idf <- as.matrix(tf_idf) %>% t() %>% as.data.frame()
tf_idf[1, 1:10]
train_clean_2$text[1]
tf_idf[1, 1:10]
head(tf_idf)
View(tf_idf)
dim(tf_idf)
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(50) %>%
rownames()
tf_idf_reducido <- tf_idf %>%
select(all_of(columnas_seleccionadas))
dim(tf_idf_reducido)
View(tf_idf)
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(100) %>%
rownames()
tf_idf_reducido <- tf_idf %>% select(all_of(columnas_seleccionadas))
dim(tf_idf_reducido)
dim(tf_idf)
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(1000) %>%
rownames()
tf_idf_reducido <- tf_idf %>% select(all_of(columnas_seleccionadas))
dim(tf_idf_reducido)
save(data, data_clean, tf_idf, tf_idf_reducido, file = "data//datos_para_modelar.RData")
save(train_clean, train_clean_2, tf_idf, tf_idf_reducido, file = "data//datos_para_modelar.RData")
?save
save(train_clean, train_clean_2, tf_idf, tf_idf_reducido, file = "data//datos_para_modelar.RData")
setwd("C:/Users/User/Documents/Big_Data/Repositorio_PS4/scripts")
setwd("C:/Users/User/Documents/Big_Data/Repositorio_PS4")
save(train_clean, train_clean_2, tf_idf, tf_idf_reducido, file = "scripts//datos_para_modelar.RData")
setwd("C:/Users/User/Documents/Big_Data/BD_Taller 4")
# Cargar pacman (contiene la función p_load)
library(pacman)
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load(tidyverse, janitor, tm, stringi, tidytext, stopwords, wordcloud2, udpipe,
ggcorrplot)
data <- read.csv("https://github.com/ignaciomsarmiento/datasets/raw/main/tripadvisor_medellin.csv", sep = ";")
glimpse(data)
data <- clean_names(data)
names(data)
# Cantidad de restaurantes
length(unique(data$nombre))
# Cantidad de comentarios por restaurante
n_comentarios = data %>%
group_by(nombre) %>%
summarise(n = n()) %>%
ungroup()
ggplot(n_comentarios, aes(x = n)) +
geom_histogram(fill = "darkblue", alpha = 0.7) +
theme_bw() +
labs(x = "Número de reseñas por restaurante", y = "Cantidad")
# La Pampa tiene full comentarios! Al parecer la gente ama la carnita asá
n_comentarios %>%
arrange(desc(n)) %>%
head()
# Veamos la distribución promedio de puntaje por restaurante
data %>%
distinct(nombre, estrellas_restaurante) %>%
ggplot(aes(x = estrellas_restaurante)) +
geom_bar(fill = "darkblue", alpha = 0.7) +
theme_bw() +
labs(x = "Estrellas por restaurante", y = "Cantidad")
# En general, la mayoría de restaurantes están top. Ahora veamos los comentarios
ggplot(data, aes(x = calificacion)) +
geom_bar(fill = "darkblue", alpha = 0.7) +
theme_bw() +
labs(x = "Estrellas por comentario", y = "Cantidad")
# Vamos a comenzar creando nuestra variable titulo + comentario
data["full_text"] = paste(data$titulo_comentario, data$comentario)
# Eliminamos las columnas que no vamos a usar
data = select(data, -titulo_comentario, -comentario)
# Vamos a limpiar esta variable
# Ponemos todo el texto en minuscula
data["full_text"] <- apply(data["full_text"], 1, tolower)
# Eliminamos numeros
data["full_text"] <- apply(data["full_text"], 1, removeNumbers)
# Eliminamos signos de puntuación
data["full_text"] <- apply(data["full_text"], 1, removePunctuation)
# Eliminamos multiples espacios en blanco
data["full_text"] <- apply(data["full_text"], 1, stripWhitespace)
# Elimiamos acentos
data["full_text"] <- apply(data["full_text"], 1, function(x)
stri_trans_general(str = x, id = "Latin-ASCII"))
# Tenemos que transformar nuestro dataframe para que quede a nivel de palabra,
# sin embargo necesitamos crear un id de comentario para no perder el rastro de
# nuestra unidad de observación.
data <- data %>%
mutate(id = row_number())
words <- data %>%
unnest_tokens(output = "word", input = "full_text")
# Eliminamos stopwords
sw <- c()
for (s in c("snowball", "stopwords-iso", "nltk")) {
temp <- get_stopwords("spanish", source = s)$word
sw <- c(sw, temp)
}
sw <- unique(sw)
sw <- unique(stri_trans_general(str = sw, id = "Latin-ASCII"))
sw <- data.frame(word = sw)
# Número de palabras antes de remover stopwords
nrow(words)
words <- words %>%
anti_join(sw, by = "word")
# Número de palabras después de remover stopwords
nrow(words)
# Veamos una nube de palabras de las 100 palabras más frecuentes
n_words <- words %>%
count(word) %>%
arrange(desc(n)) %>%
head(100)
wordcloud2(data = n_words)
# Vamos a lematizar
# udpipe::udpipe_download_model('spanish')
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
# Vamos a lematizar
udpipe::udpipe_download_model('spanish')
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- words %>%
distinct(word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
udpipe_results <- as_tibble(udpipe_results)
udpipe_results <- udpipe_results %>%
select(token, lemma) %>%
rename("word" = "token")
words <- words %>%
left_join(udpipe_results, by = "word", multiple = "all")
words[is.na(words$lemma), "lemma"] <- words[is.na(words$lemma), "word"]
# Veamos las palabras menos comunes
words %>%
count(lemma) %>%
arrange(desc(n)) %>%
tail(100)
# Para que la matriz TF-IDF no sea GIGANTE vamos a eliminar todas las palabras que aparezcan menos de 10 veces. ¿Por qué 10? No sé, me la estoy jugando, con esto pueden jugar ustedes después
palabras_eliminar <- words %>%
count(lemma) %>%
filter(n < 10)
words <- words %>%
anti_join(palabras_eliminar, by = "lemma")
# Volvemos a nuestro formato original. Comentario por fila
data_clean <- words %>%
group_by(nombre, estrellas_restaurante, usuario, calificacion, id) %>%
summarise(comentario = str_c(lemma, collapse = " ")) %>%
ungroup()
# Se eliminaron dos comentarios. Estos solo estaban compuestos por stopwords y
# espacios.
setdiff(data$id, data_clean$id)
data[c(1490, 1491),]
# Creamos un corpus
tm_corpus <- Corpus(VectorSource(x = data_clean$comentario))
str(tm_corpus)
# Creamos TF-IDF
tf_idf <- TermDocumentMatrix(tm_corpus,
control = list(weighting = weightTfIdf))
tf_idf <- as.matrix(tf_idf) %>%
t() %>%
as.data.frame()
# Revisamos que todo este ok
data_clean$comentario[1]
tf_idf[1, 1:10]
head(tf_idf)
dim(tf_idf)
# Nos vamos a quedar con las columnas que tengan los valores mas altos
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(50) %>%
rownames()
tf_idf_reducido <- tf_idf %>%
select(all_of(columnas_seleccionadas))
save(data, data_clean, tf_idf, tf_idf_reducido,
file = "data//datos_para_modelar.RData")
wordcloud2(data = n_words)
wordcloud2(data = n_words)
wordcloud2(data = n_words)
View(data)
View(data)
wordcloud2(train_cloud$word, freq = train_cloud$n, min.freq = 80, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
n_words
wordcloud(cloud_lopez$word, freq = cloud_lopez$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
cloud_lopez
wordcloud2(data = cloud_lopez)
View(cloud_lopez)
View(n_words)
cloud_lopez <- train_cloud %>% select(word, n) %>% filter(name == "Lopez") %>% arrange(desc(n)) %>% head(50)
cloud_lopez <- train_cloud  %>% filter(name == "Lopez")%>% select(word, n) %>% arrange(desc(n)) %>% head(50)
cloud_lopez <- train_cloud  %>% filter(name == "Lopez") %>% select(word, n) %>% arrange(desc(n)) %>% head(50)
cloud_uribe <- train_cloud %>% filter(name == "Uribe") %>% select(word, n) %>% arrange(desc(n)) %>% head(50)
cloud_petro <- train_cloud %>% filter(name == "Petro") %>% select(word, n) %>% arrange(desc(n)) %>% head(50)
wordcloud2(data = cloud_lopez)
wordcloud2(data = cloud_petro)
View(udpipe_results)
View(words)
tf_idf
dim(tf_idf)
colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(50) %>%
rownames()
#------------------------------------------------------------------------------#
#
#                                PROBLEM SET 4
#
#                            "Predicting Tweets"
#
#       Grupo 5:  Isabella Mendez Pedraza.
#                 Manuela Ojeda Ojeda.
#                 Juan Sebastian Tellez Melo.
#                 Andres Mauricio Palacio Lugo.
#
#------------------------------------------------------------------------------#
#Cargamos librerías - Verificamos que ninguna genere conflicto
library(pacman)
p_load(dplyr, tidyverse, tm, textir, tidytext, wordcloud, SentimentAnalysis,
udpipe, syuzhet,stringi,stopwords,textstem,topicmodels, rio, caret, sentimentr,
janitor, wordcloud2, udpipe,ggcorrplot)
rm(list=ls())
#Importamos los datos
test   <- import("https://raw.githubusercontent.com/AndresMPL/Repositorio_PS4/main/datasets/test.csv")
train  <- import("https://raw.githubusercontent.com/AndresMPL/Repositorio_PS4/main/datasets/train.csv")
glimpse(train)
glimpse(test)
#Limpiamos el texto--------------------------------------------------------
train$name <- as.factor(train$name)
levels(train$name)
train_clean <- train %>%
mutate(corpus = stri_trans_general(text, id = "Latin-ASCII")) %>%
mutate(corpus = removeNumbers(text)) %>%
mutate(corpus = removePunctuation(corpus)) %>%
mutate(corpus = tolower(corpus)) %>%
mutate(corpus = stripWhitespace(corpus)) %>%
mutate(corpus = str_replace_all(corpus, "[^[:alnum:]\\s]", ""))  %>%
mutate(corpus = gsub("[^[:alpha:]#\\s]", " ", corpus)) %>%
mutate(corpus = removeWords(corpus, stopwords("es"))) %>%
mutate(corpus = gsub("\\d+", " ", corpus)) %>%
mutate(corpus = gsub("http\\S+", " ", corpus)) %>%
mutate(corpus = gsub("www\\S+", " ", corpus)) %>%
mutate(corpus = gsub("\\b\\w{1,2}\\b", " ", corpus)) %>%
mutate(corpus = gsub("\\s+", " ", corpus))
#Tokens----
train_token <- train_clean %>% unnest_tokens("word", corpus)
train_token %>% count(word, sort = TRUE) %>% head()
sw <- c()
for (s in c("snowball", "stopwords-iso", "nltk")) {
temp <- get_stopwords("spanish", source = s)$word
sw <- c(sw, temp)
}
sw <- unique(sw)
sw <- unique(stri_trans_general(str = sw, id = "Latin-ASCII"))
sw <- data.frame(word = sw)
nrow(train_token)
train_token <- train_token %>% anti_join(sw, by = "word")
nrow(train_token)
#------------------------------------------------------------------------------#
#
#                              2 - ANALISIS
#
#------------------------------------------------------------------------------#
n_tweets = train %>% group_by(name) %>% summarise(n = n()) %>% ungroup()
autor_tweets <- ggplot(n_tweets, aes(name, n, fill = name)) +
geom_col() + geom_text(aes(label = n), vjust = -1, colour = "black") +
ylim(c(0, 2700)) + theme_bw() +
scale_fill_manual(values = c("cadetblue3", "#CCEDB1", "#FFB90F")) +
theme(legend.position = "none") +
labs(x = "Autor", y = "Número de tweets")
##Histograma de palabras----
train_cloud <- train_token %>% count(name, word) %>% group_by(name) %>% ungroup()
autor_top_word <- train_cloud %>%
group_by(name) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
arrange(name, -n)
imagen_top_autor <- ggplot(autor_top_word, aes(y = reorder(word, n), x = n, fill = factor(name))) +
geom_bar(stat = "identity") +
ggtitle("Términos más frecuentes") +
ylab("Términos") +
facet_wrap(~ name, scales = "free") +
xlab("Frecuencia") +
theme(legend.position = "none")
imagen_top_autor
##Nube de palabras----
train_cloud <- train_token %>% count(name, word) %>% group_by(name) %>% ungroup()
table(train_cloud$name) %>% as.data.frame()
cloud_lopez <- train_cloud  %>% filter(name == "Lopez") %>% select(word, n) %>% arrange(desc(n)) %>% head(50)
cloud_uribe <- train_cloud %>% filter(name == "Uribe") %>% select(word, n) %>% arrange(desc(n)) %>% head(50)
cloud_petro <- train_cloud %>% filter(name == "Petro") %>% select(word, n) %>% arrange(desc(n)) %>% head(50)
wordcloud(train_cloud$word, freq = train_cloud$n, min.freq = 80, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(cloud_lopez$word, freq = cloud_lopez$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(cloud_uribe$word, freq = cloud_uribe$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud(cloud_petro$word, freq = cloud_petro$n, colors= brewer.pal(8, "Dark2"),random.order = FALSE)
wordcloud2(data = cloud_lopez)
wordcloud2(data = cloud_uribe)
wordcloud2(data = cloud_petro)
##Lemma
#udpipe::udpipe_download_model('spanish')
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- train_token %>% distinct(word = train_token$word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
udpipe_results <- as_tibble(udpipe_results)
udpipe_results <- udpipe_results %>% select(token, lemma) %>% rename("word" = "token")
train_token <- train_token %>% left_join(udpipe_results, by = "word", multiple = "all")
train_token[is.na(train_token$lemma), "lemma"] <- train_token[is.na(train_token$lemma), "word"]
train_token %>% count(lemma) %>% arrange(desc(n)) %>% tail(100)
palabras_eliminar <- train_token %>% count(lemma) %>% filter(n < 10)
train_token <- train_token %>% anti_join(palabras_eliminar, by = "lemma")
train_clean_2 <- train_token %>%
group_by(name, id) %>%
summarise(text = str_c(lemma, collapse = " ")) %>%
ungroup()
diferencia <- setdiff(train_clean$id, train_clean_2$id) %>% as.data.frame()
dif <- nrow(diferencia)
inicial <- nrow(train_clean)
final <- nrow(train_clean_2)
inicial - final #debe ser igual a dif
train_clean %>% filter (id == "ce1464da0f03a61f2659947b") #Ejemplo para validar
#Matriz de Términos----
tm_corpus <- Corpus(VectorSource(x = train_clean_2$text))
str(tm_corpus)
tf_idf <- TermDocumentMatrix(tm_corpus, control = list(weighting = weightTfIdf))
tf_idf <- as.matrix(tf_idf) %>% t() %>% as.data.frame()
train_clean_2$text[1]
tf_idf[1, 1:10]
head(tf_idf)
dim(tf_idf)
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(1000) %>%
rownames()
tf_idf_reducido <- tf_idf %>% select(all_of(columnas_seleccionadas))
dim(tf_idf_reducido)
save(train_clean, train_clean_2, tf_idf, tf_idf_reducido, file = "scripts//datos_para_modelar.RData")
setwd("C:/Users/User/Documents/Big_Data/Repositorio_PS4")
save(train_clean, train_clean_2, tf_idf, tf_idf_reducido, file = "scripts//datos_para_modelar.RData")
View(train_clean_2)
sum(is.na(train_clean_2$name))
Y <- train_clean_2$name
Y <- to_categorical(Y)
reticulate::install_miniconda()
reticulate::install_miniconda()
Y <- to_categorical(Y)
Y <- to_categorical(Y)
install.packages('keras')
install.packages("keras")
library(keras)
Y <- to_categorical(Y)
reticulate::install_miniconda()
Y <- to_categorical(Y)
table(train_clean_2$name)
head(Y)
Y <- to_categorical(Y)
str(Y)
Y <- train_clean_2$name
str(Y)
