layer_dense(units = 6, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model2)
model2 %>% compile(optimizer = 'adagrad', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^8, validation_split = 0.2)
history_plot2 <- plot(history2)
history_plot2
model2 %>% evaluate(X_test, y_test)
y_hat2 <- model2 %>% predict(X_test) %>% k_argmax()
confusionMatrix(data = factor(as.numeric(y_hat2), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(1000) %>%
rownames()
tf_idf_reducido <- tf_idf %>% select(all_of(columnas_seleccionadas))
dim(tf_idf_reducido)
sum(is.na(train_clean_2$name))
train_clean_2$name2 <- train_clean_2$name #Guardamos el nombre en una variable nueva
train_clean_2$name2 <- ifelse(train_clean_2$name2 == "Lopez", 1, ifelse(train_clean_2$name2 == "Petro", 2, 3))
train_clean_2$name2 <- as.factor(train_clean_2$name2)
table(train_clean_2$name2)
levels(train_clean_2$name2)
Y <- train_clean_2$name2
Y <- to_categorical(Y)
class(Y)
head(Y)
dim(Y)
colSums(Y)
X <- as.matrix(tf_idf_reducido)
class(X)
dim(X)
set.seed(10101)
n <- nrow(train_clean_2)
data_rows <- floor(0.70 * n)
train_indices <- sample(1:n, data_rows)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- Y[train_indices, ]
y_test <- Y[-train_indices, ]
n_h = nrow(X_train)/(2*(ncol(X_train) + 5))
rm(model)
model <- keras_model_sequential()
model %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model)
model %>% compile(optimizer = 'sgd', loss = 'mean_squared_error', metrics = c('accuracy'))
history <- model %>% fit(X_train, y_train, epochs = 100, batch_size = 2^8, validation_split = 0.2)
history_plot <- plot(history)
history_plot
model %>% evaluate(X_test, y_test)
y_hat <- model %>% predict(X_test) %>% k_argmax()
confusionMatrix(data = factor(as.numeric(y_hat), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model2)
model2 <- keras_model_sequential()
model2 %>%
layer_dense(units = 6, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model2)
model2 %>% compile(optimizer = 'adagrad', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^8, validation_split = 0.2)
history_plot2 <- plot(history2)
history_plot2
model2 %>% evaluate(X_test, y_test)
y_hat2 <- model2 %>% predict(X_test) %>% k_argmax()
confusionMatrix(data = factor(as.numeric(y_hat2), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model2)
model2 <- keras_model_sequential()
model2 %>%
layer_dense(units = 6, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model2)
model2 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^8, validation_split = 0.2)
history_plot2 <- plot(history2)
history_plot2
model2 %>% evaluate(X_test, y_test)
y_hat2 <- model2 %>% predict(X_test) %>% k_argmax()
confusionMatrix(data = factor(as.numeric(y_hat2), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model2)
model2 <- keras_model_sequential()
model2 %>%
layer_dense(units = 2, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model2)
model2 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^8, validation_split = 0.2)
history_plot2 <- plot(history2)
history_plot2
model2 %>% evaluate(X_test, y_test)
rm(model2)
model2 <- keras_model_sequential()
model2 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model2)
model2 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^8, validation_split = 0.2)
rm(model2)
model2 <- keras_model_sequential()
model2 %>%
layer_dense(units = 2, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model2)
model2 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 200, batch_size = 2^8, validation_split = 0.2)
history_plot2 <- plot(history2)
history_plot2
model2 %>% evaluate(X_test, y_test)
y_hat2 <- model2 %>% predict(X_test) %>% k_argmax()
confusionMatrix(data = factor(as.numeric(y_hat2), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
#Prueba 1-----------------------------------------------------------------------
tokenizer <- text_tokenizer(num_words = ncol(X_train))
tokenizer %>% fit_text_tokenizer(test_final$text)
x_prueba <- texts_to_matrix(tokenizer, test_final$text, mode ="tfidf")
y_hat_test <- model2 %>% predict(x_prueba) %>% k_argmax()
predicho <- factor(as.numeric(y_hat_test))
resultados <- data.frame(id = test_final$id, name = predicho)
resultados$name <- ifelse(resultados$name == 1, "Lopez", ifelse(resultados$name == 2, "Petro", "Uribe"))
resultados$name <- as.factor(resultados$name)
write.table(resultados, "submission_1.csv", row.names = FALSE, quote=FALSE, sep = ",")
View(train_token)
View(train_clean)
train_token <- train_clean %>% unnest_tokens("word", corpus)
View(train_token)
train_token %>% count(word, sort = TRUE) %>% head()
sw <- c()
for (s in c("snowball", "stopwords-iso", "nltk")) {
temp <- get_stopwords("spanish", source = s)$word
sw <- c(sw, temp)
}
sw <- unique(sw)
sw <- unique(stri_trans_general(str = sw, id = "Latin-ASCII"))
sw <- data.frame(word = sw)
nrow(train_token)
train_token <- train_token %>% anti_join(sw, by = "word")
View(train_token)
#udpipe::udpipe_download_model('spanish')
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- train_token %>% distinct(word = train_token$word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
udpipe_results <- as_tibble(udpipe_results)
udpipe_results <- udpipe_results %>% select(token, lemma) %>% rename("word" = "token")
dpipe::udpipe_download_model('spanish')
udpipe::udpipe_download_model('spanish')
#udpipe::udpipe_download_model('spanish')
model <- udpipe_load_model(file = "spanish-gsd-ud-2.5-191206.udpipe")
palabras_unicas <- train_token %>% distinct(word = train_token$word)
udpipe_results <- udpipe_annotate(model, x = palabras_unicas$word)
udpipe_results <- as_tibble(udpipe_results)
udpipe_results <- udpipe_results %>% select(token, lemma) %>% rename("word" = "token")
train_token <- train_token %>% left_join(udpipe_results, by = "word", multiple = "all")
View(train_token)
View(palabras_unicas)
View(udpipe_results)
View(udpipe_results)
train_token <- train_token %>% left_join(udpipe_results, by = "word", multiple = "all")
View(train_token)
View(train_token)
?TermDocumentMatrix
tf_idf2 <- TermDocumentMatrix(tm_corpus2, control = list(weighting = weightTfIdf))
tf_idf2 <- as.matrix(tf_idf2) %>% t() %>% as.data.frame()
columnas_seleccionadas2 <- colSums(tf_idf2) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(ncol(X_train)) %>%
rownames()
tf_idf_reducido2 <- tf_idf2 %>% select(all_of(columnas_seleccionadas2))
Z <- as.matrix(tf_idf_reducido2)
class(Z)
dim(Z)
y_hat_test <- model2 %>% predict(Z) %>% k_argmax()
predicho <- factor(as.numeric(y_hat_test))
resultados <- data.frame(id = test_final$id, name = predicho)
View(resultados)
resultados$name <- ifelse(resultados$name == 1, "Lopez", ifelse(resultados$name == 2, "Petro", "Uribe"))
resultados$name <- as.factor(resultados$name)
write.table(resultados, "submission_1.csv", row.names = FALSE, quote=FALSE, sep = ",")
ncol(X_train)
model2 <- keras_model_sequential()
model2 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.6) %>%
layer_dense(units = 4, activation = 'relu')
summary(model2)
model2 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 200, batch_size = 2^8, validation_split = 0.2)
history_plot2 <- plot(history2)
history_plot2
model2 %>% evaluate(X_test, y_test)
rm(model3)
model2 <- keras_model_sequential()
model2 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.6) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model2)
model2 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model2 %>% fit(X_train, y_train, epochs = 200, batch_size = 2^8, validation_split = 0.2)
history_plot2 <- plot(history2)
history_plot2
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.6) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history2 <- model3 %>% fit(X_train, y_train, epochs = 200, batch_size = 2^8, validation_split = 0.2)
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.6) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 20, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 20, batch_size = 2^8, validation_split = 0.2)
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('accuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^16, validation_split = 0.2)
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^16, validation_split = 0.2)
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(500) %>%
rownames()
tf_idf_reducido <- tf_idf %>% select(all_of(columnas_seleccionadas))
dim(tf_idf_reducido)
sum(is.na(train_clean_2$name))
train_clean_2$name2 <- train_clean_2$name #Guardamos el nombre en una variable nueva
train_clean_2$name2 <- ifelse(train_clean_2$name2 == "Lopez", 1, ifelse(train_clean_2$name2 == "Petro", 2, 3))
train_clean_2$name2 <- as.factor(train_clean_2$name2)
table(train_clean_2$name2)
levels(train_clean_2$name2)
Y <- train_clean_2$name2
Y <- to_categorical(Y)
class(Y)
head(Y)
dim(Y)
colSums(Y)
X <- as.matrix(tf_idf_reducido)
class(X)
dim(X)
set.seed(10101)
n <- nrow(train_clean_2)
data_rows <- floor(0.70 * n)
train_indices <- sample(1:n, data_rows)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- Y[train_indices, ]
y_test <- Y[-train_indices, ]
n_h = nrow(X_train)/(2*(ncol(X_train) + 5))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 6, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^16, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(100) %>%
rownames()
sum(is.na(train_clean_2$name))
train_clean_2$name2 <- train_clean_2$name #Guardamos el nombre en una variable nueva
train_clean_2$name2 <- ifelse(train_clean_2$name2 == "Lopez", 1, ifelse(train_clean_2$name2 == "Petro", 2, 3))
train_clean_2$name2 <- as.factor(train_clean_2$name2)
table(train_clean_2$name2)
levels(train_clean_2$name2)
Y <- train_clean_2$name2
Y <- to_categorical(Y)
class(Y)
head(Y)
dim(Y)
colSums(Y)
X <- as.matrix(tf_idf_reducido)
class(X)
dim(X)
set.seed(10101)
n <- nrow(train_clean_2)
data_rows <- floor(0.70 * n)
train_indices <- sample(1:n, data_rows)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- Y[train_indices, ]
y_test <- Y[-train_indices, ]
n_h = nrow(X_train)/(2*(ncol(X_train) + 5))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 2, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 20, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
columnas_seleccionadas <- colSums(tf_idf) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(2000) %>%
rownames()
tf_idf_reducido <- tf_idf %>% select(all_of(columnas_seleccionadas))
dim(tf_idf_reducido)
sum(is.na(train_clean_2$name))
train_clean_2$name2 <- train_clean_2$name #Guardamos el nombre en una variable nueva
train_clean_2$name2 <- ifelse(train_clean_2$name2 == "Lopez", 1, ifelse(train_clean_2$name2 == "Petro", 2, 3))
train_clean_2$name2 <- as.factor(train_clean_2$name2)
table(train_clean_2$name2)
levels(train_clean_2$name2)
Y <- train_clean_2$name2
Y <- to_categorical(Y)
class(Y)
head(Y)
dim(Y)
colSums(Y)
X <- as.matrix(tf_idf_reducido)
class(X)
dim(X)
set.seed(10101)
n <- nrow(train_clean_2)
data_rows <- floor(0.70 * n)
train_indices <- sample(1:n, data_rows)
X_train <- X[train_indices, ]
X_test <- X[-train_indices, ]
y_train <- Y[train_indices, ]
y_test <- Y[-train_indices, ]
n_h = nrow(X_train)/(2*(ncol(X_train) + 5))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 2, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 20, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
#library(caret)
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 3, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 20, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
#library(caret)
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 4, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 20, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
#library(caret)
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 10, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 20, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
#library(caret)
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 10, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 100, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
#library(caret)
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model3)
model3 <- keras_model_sequential()
model3 %>%
layer_dense(units = 10, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model3)
model3 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model3 %>% fit(X_train, y_train, epochs = 500, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model3 %>% evaluate(X_test, y_test)
y_hat3 <- model3 %>% predict(X_test) %>% k_argmax()
#library(caret)
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
rm(model4)
model4 <- keras_model_sequential()
model4 %>%
layer_dense(units = 1, activation = 'relu', input_shape = ncol(X_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 4, activation = 'softmax')
summary(model4)
model4 %>% compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = c('CategoricalAccuracy'))
history3 <- model4 %>% fit(X_train, y_train, epochs = 200, batch_size = 2^8, validation_split = 0.2)
history_plot3 <- plot(history3)
history_plot3
model4 %>% evaluate(X_test, y_test)
y_hat4 <- model4 %>% predict(X_test) %>% k_argmax()
#library(caret)
confusionMatrix(data = factor(as.numeric(y_hat4), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
confusionMatrix(data = factor(as.numeric(y_hat3), levels = 1:3),
reference = factor(train_clean_2$name2[-train_indices], levels = 1:3))
columnas_seleccionadas2 <- colSums(tf_idf2) %>%
data.frame() %>%
arrange(desc(.)) %>%
head(ncol(X_train)) %>%
rownames()
tf_idf_reducido2 <- tf_idf2 %>% select(all_of(columnas_seleccionadas2))
dim(tf_idf_reducido2)
Z <- as.matrix(tf_idf_reducido2)
class(Z)
dim(Z)
y_hat_test <- model3 %>% predict(Z) %>% k_argmax()
predicho <- factor(as.numeric(y_hat_test))
resultados <- data.frame(id = test_final$id, name = predicho)
resultados$name <- ifelse(resultados$name == 1, "Lopez", ifelse(resultados$name == 2, "Petro", "Uribe"))
resultados$name <- as.factor(resultados$name)
write.table(resultados, "submission_1.csv", row.names = FALSE, quote=FALSE, sep = ",")
